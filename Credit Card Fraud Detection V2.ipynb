{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credit Card Fraud Detection V2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning Style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alex Martishius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from tabulate import tabulate\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#working dir\n",
    "os.chdir('C:\\\\Users\\\\alexm\\\\Desktop\\\\Kaggle')\n",
    "#import data\n",
    "raw_ds = pd.read_csv('creditcard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>284807.000000</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>94813.859575</td>\n",
       "      <td>3.919560e-15</td>\n",
       "      <td>5.688174e-16</td>\n",
       "      <td>-8.769071e-15</td>\n",
       "      <td>2.782312e-15</td>\n",
       "      <td>-1.552563e-15</td>\n",
       "      <td>2.010663e-15</td>\n",
       "      <td>-1.694249e-15</td>\n",
       "      <td>-1.927028e-16</td>\n",
       "      <td>-3.137024e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>1.537294e-16</td>\n",
       "      <td>7.959909e-16</td>\n",
       "      <td>5.367590e-16</td>\n",
       "      <td>4.458112e-15</td>\n",
       "      <td>1.453003e-15</td>\n",
       "      <td>1.699104e-15</td>\n",
       "      <td>-3.660161e-16</td>\n",
       "      <td>-1.206049e-16</td>\n",
       "      <td>88.349619</td>\n",
       "      <td>0.001727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>47488.145955</td>\n",
       "      <td>1.958696e+00</td>\n",
       "      <td>1.651309e+00</td>\n",
       "      <td>1.516255e+00</td>\n",
       "      <td>1.415869e+00</td>\n",
       "      <td>1.380247e+00</td>\n",
       "      <td>1.332271e+00</td>\n",
       "      <td>1.237094e+00</td>\n",
       "      <td>1.194353e+00</td>\n",
       "      <td>1.098632e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>7.345240e-01</td>\n",
       "      <td>7.257016e-01</td>\n",
       "      <td>6.244603e-01</td>\n",
       "      <td>6.056471e-01</td>\n",
       "      <td>5.212781e-01</td>\n",
       "      <td>4.822270e-01</td>\n",
       "      <td>4.036325e-01</td>\n",
       "      <td>3.300833e-01</td>\n",
       "      <td>250.120109</td>\n",
       "      <td>0.041527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.640751e+01</td>\n",
       "      <td>-7.271573e+01</td>\n",
       "      <td>-4.832559e+01</td>\n",
       "      <td>-5.683171e+00</td>\n",
       "      <td>-1.137433e+02</td>\n",
       "      <td>-2.616051e+01</td>\n",
       "      <td>-4.355724e+01</td>\n",
       "      <td>-7.321672e+01</td>\n",
       "      <td>-1.343407e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.483038e+01</td>\n",
       "      <td>-1.093314e+01</td>\n",
       "      <td>-4.480774e+01</td>\n",
       "      <td>-2.836627e+00</td>\n",
       "      <td>-1.029540e+01</td>\n",
       "      <td>-2.604551e+00</td>\n",
       "      <td>-2.256568e+01</td>\n",
       "      <td>-1.543008e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>54201.500000</td>\n",
       "      <td>-9.203734e-01</td>\n",
       "      <td>-5.985499e-01</td>\n",
       "      <td>-8.903648e-01</td>\n",
       "      <td>-8.486401e-01</td>\n",
       "      <td>-6.915971e-01</td>\n",
       "      <td>-7.682956e-01</td>\n",
       "      <td>-5.540759e-01</td>\n",
       "      <td>-2.086297e-01</td>\n",
       "      <td>-6.430976e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.283949e-01</td>\n",
       "      <td>-5.423504e-01</td>\n",
       "      <td>-1.618463e-01</td>\n",
       "      <td>-3.545861e-01</td>\n",
       "      <td>-3.171451e-01</td>\n",
       "      <td>-3.269839e-01</td>\n",
       "      <td>-7.083953e-02</td>\n",
       "      <td>-5.295979e-02</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>84692.000000</td>\n",
       "      <td>1.810880e-02</td>\n",
       "      <td>6.548556e-02</td>\n",
       "      <td>1.798463e-01</td>\n",
       "      <td>-1.984653e-02</td>\n",
       "      <td>-5.433583e-02</td>\n",
       "      <td>-2.741871e-01</td>\n",
       "      <td>4.010308e-02</td>\n",
       "      <td>2.235804e-02</td>\n",
       "      <td>-5.142873e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.945017e-02</td>\n",
       "      <td>6.781943e-03</td>\n",
       "      <td>-1.119293e-02</td>\n",
       "      <td>4.097606e-02</td>\n",
       "      <td>1.659350e-02</td>\n",
       "      <td>-5.213911e-02</td>\n",
       "      <td>1.342146e-03</td>\n",
       "      <td>1.124383e-02</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>139320.500000</td>\n",
       "      <td>1.315642e+00</td>\n",
       "      <td>8.037239e-01</td>\n",
       "      <td>1.027196e+00</td>\n",
       "      <td>7.433413e-01</td>\n",
       "      <td>6.119264e-01</td>\n",
       "      <td>3.985649e-01</td>\n",
       "      <td>5.704361e-01</td>\n",
       "      <td>3.273459e-01</td>\n",
       "      <td>5.971390e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.863772e-01</td>\n",
       "      <td>5.285536e-01</td>\n",
       "      <td>1.476421e-01</td>\n",
       "      <td>4.395266e-01</td>\n",
       "      <td>3.507156e-01</td>\n",
       "      <td>2.409522e-01</td>\n",
       "      <td>9.104512e-02</td>\n",
       "      <td>7.827995e-02</td>\n",
       "      <td>77.165000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>172792.000000</td>\n",
       "      <td>2.454930e+00</td>\n",
       "      <td>2.205773e+01</td>\n",
       "      <td>9.382558e+00</td>\n",
       "      <td>1.687534e+01</td>\n",
       "      <td>3.480167e+01</td>\n",
       "      <td>7.330163e+01</td>\n",
       "      <td>1.205895e+02</td>\n",
       "      <td>2.000721e+01</td>\n",
       "      <td>1.559499e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>2.720284e+01</td>\n",
       "      <td>1.050309e+01</td>\n",
       "      <td>2.252841e+01</td>\n",
       "      <td>4.584549e+00</td>\n",
       "      <td>7.519589e+00</td>\n",
       "      <td>3.517346e+00</td>\n",
       "      <td>3.161220e+01</td>\n",
       "      <td>3.384781e+01</td>\n",
       "      <td>25691.160000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Time            V1            V2            V3            V4  \\\n",
       "count  284807.000000  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean    94813.859575  3.919560e-15  5.688174e-16 -8.769071e-15  2.782312e-15   \n",
       "std     47488.145955  1.958696e+00  1.651309e+00  1.516255e+00  1.415869e+00   \n",
       "min         0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00   \n",
       "25%     54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01   \n",
       "50%     84692.000000  1.810880e-02  6.548556e-02  1.798463e-01 -1.984653e-02   \n",
       "75%    139320.500000  1.315642e+00  8.037239e-01  1.027196e+00  7.433413e-01   \n",
       "max    172792.000000  2.454930e+00  2.205773e+01  9.382558e+00  1.687534e+01   \n",
       "\n",
       "                 V5            V6            V7            V8            V9  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean  -1.552563e-15  2.010663e-15 -1.694249e-15 -1.927028e-16 -3.137024e-15   \n",
       "std    1.380247e+00  1.332271e+00  1.237094e+00  1.194353e+00  1.098632e+00   \n",
       "min   -1.137433e+02 -2.616051e+01 -4.355724e+01 -7.321672e+01 -1.343407e+01   \n",
       "25%   -6.915971e-01 -7.682956e-01 -5.540759e-01 -2.086297e-01 -6.430976e-01   \n",
       "50%   -5.433583e-02 -2.741871e-01  4.010308e-02  2.235804e-02 -5.142873e-02   \n",
       "75%    6.119264e-01  3.985649e-01  5.704361e-01  3.273459e-01  5.971390e-01   \n",
       "max    3.480167e+01  7.330163e+01  1.205895e+02  2.000721e+01  1.559499e+01   \n",
       "\n",
       "       ...           V21           V22           V23           V24  \\\n",
       "count  ...  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   ...  1.537294e-16  7.959909e-16  5.367590e-16  4.458112e-15   \n",
       "std    ...  7.345240e-01  7.257016e-01  6.244603e-01  6.056471e-01   \n",
       "min    ... -3.483038e+01 -1.093314e+01 -4.480774e+01 -2.836627e+00   \n",
       "25%    ... -2.283949e-01 -5.423504e-01 -1.618463e-01 -3.545861e-01   \n",
       "50%    ... -2.945017e-02  6.781943e-03 -1.119293e-02  4.097606e-02   \n",
       "75%    ...  1.863772e-01  5.285536e-01  1.476421e-01  4.395266e-01   \n",
       "max    ...  2.720284e+01  1.050309e+01  2.252841e+01  4.584549e+00   \n",
       "\n",
       "                V25           V26           V27           V28         Amount  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  284807.000000   \n",
       "mean   1.453003e-15  1.699104e-15 -3.660161e-16 -1.206049e-16      88.349619   \n",
       "std    5.212781e-01  4.822270e-01  4.036325e-01  3.300833e-01     250.120109   \n",
       "min   -1.029540e+01 -2.604551e+00 -2.256568e+01 -1.543008e+01       0.000000   \n",
       "25%   -3.171451e-01 -3.269839e-01 -7.083953e-02 -5.295979e-02       5.600000   \n",
       "50%    1.659350e-02 -5.213911e-02  1.342146e-03  1.124383e-02      22.000000   \n",
       "75%    3.507156e-01  2.409522e-01  9.104512e-02  7.827995e-02      77.165000   \n",
       "max    7.519589e+00  3.517346e+00  3.161220e+01  3.384781e+01   25691.160000   \n",
       "\n",
       "               Class  \n",
       "count  284807.000000  \n",
       "mean        0.001727  \n",
       "std         0.041527  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.000000  \n",
       "max         1.000000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Quick describe\n",
    "raw_ds.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train/test/va split;\n",
    "#Define the master X and y by selecting out the response variable and dropping it for X\n",
    "\n",
    "#First drop 'time' because it is not relevant;\n",
    "raw=raw_ds.drop(columns=['Time'])\n",
    "#split into X and y\n",
    "y = raw[['Class']]\n",
    "X = raw.drop(columns=['Class'])\n",
    "\n",
    "#If we want train/validation/test, we need 2 calls to train_test_split\n",
    "#This one splits into 80% train & 20% test.\n",
    "Xtr, Xte, ytr, yte=train_test_split(X, y, test_size=0.2, random_state=19)\n",
    "#This one splits into 80*75% train (60%) & 80*25% (20%) validation\n",
    "Xtr, Xva, ytr, yva=train_test_split(Xtr, ytr, test_size=0.25, random_state=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtr shape: (170883, 29)\n",
      "ytr shape: (170883, 1)\n",
      "Xva shape: (56962, 29)\n",
      "yva shape: (56962, 1)\n",
      "Xte shape: (56962, 29)\n",
      "yte shape: (56962, 1)\n"
     ]
    }
   ],
   "source": [
    "#No need to standardize - inputs are already principal components;\n",
    "#Shape of training set\n",
    "print(\"Xtr shape:\",Xtr.shape)\n",
    "print(\"ytr shape:\",ytr.shape)\n",
    "\n",
    "#Shape of validaiton set\n",
    "print(\"Xva shape:\",Xva.shape)\n",
    "print(\"yva shape:\",yva.shape)\n",
    "\n",
    "#Shape of testing set\n",
    "print(\"Xte shape:\",Xte.shape)\n",
    "print(\"yte shape:\",yte.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexm\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Model        |      Acc |      AUC |   Precision |   Recall |   F1 Score |\n",
      "|--------------+----------+----------+-------------+----------+------------|\n",
      "| Wtd Logistic | 0.999368 | 0.914833 |    0.813725 |     0.83 |   0.821782 |\n"
     ]
    }
   ],
   "source": [
    "#The benchmark is the \"best\" logistic regression model:  C=0.38 and W = {0:11,1:89}\n",
    "c=0.38\n",
    "w = {0:11,1:89}\n",
    "\n",
    "clf = LogisticRegression(penalty='l2',class_weight=w,C=c,solver='lbfgs',max_iter=1500).fit(Xtr, ytr)\n",
    "ypreds = clf.predict(Xte)\n",
    "\n",
    "f1 = f1_score(yte,ypreds)\n",
    "prec = precision_score(yte,ypreds)\n",
    "acc = accuracy_score(yte,ypreds)\n",
    "rec = recall_score(yte,ypreds)\n",
    "auc = roc_auc_score(yte,ypreds)\n",
    "\n",
    "logit_stats = ['Wtd Logistic',acc,auc,prec,rec,f1]\n",
    "table_headers = ['Model','Acc','AUC','Precision','Recall','F1 Score']\n",
    "\n",
    "print(tabulate([logit_stats], headers=table_headers, tablefmt='orgtbl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   Total Trans |   Total Fraud |   Recalled Fraud |   Slipped Fraud |   Recall % |   FalsePos Cost |   Value Added |\n",
      "|---------------+---------------+------------------+-----------------+------------+-----------------+---------------|\n",
      "|   5.02639e+06 |       13238.9 |          10287.8 |         2951.01 |   0.777095 |             950 |       9337.84 |\n"
     ]
    }
   ],
   "source": [
    "def fraud_impact(amounts,yhat,y,falcost=0,show=False,bring=True):\n",
    "    total_amt = np.sum(amounts)\n",
    "    total_fraud = np.sum(amounts*y)\n",
    "    recalled_fraud = np.sum(amounts*y*yhat)\n",
    "    slipped_fraud = total_fraud-recalled_fraud\n",
    "    false_positive_cost = np.sum((yhat*(1-y))*falcost)\n",
    "    recall_percentage = recalled_fraud/total_fraud\n",
    "    value = recalled_fraud-false_positive_cost\n",
    "    \n",
    "    table_headers = ['Total Trans','Total Fraud','Recalled Fraud','Slipped Fraud','Recall %','FalsePos Cost','Value Added']\n",
    "    table_values = [total_amt,total_fraud,recalled_fraud,slipped_fraud,recall_percentage,false_positive_cost,value]\n",
    "    \n",
    "    if show:\n",
    "        print(tabulate([table_values], headers=table_headers, tablefmt='orgtbl'))\n",
    "    if bring:\n",
    "        return table_values\n",
    "    else:\n",
    "        return\n",
    "    \n",
    "dollar_imp = pd.DataFrame(columns=['Amount','Predicted','Actual'])\n",
    "dollar_imp[['Amount']]=Xte[['Amount']]\n",
    "dollar_imp[['Predicted']]=ypreds\n",
    "dollar_imp[['Actual']]=yte\n",
    "dollar_imp.reset_index(drop=True,inplace=True)\n",
    "\n",
    "amt = np.array(dollar_imp[['Amount']])\n",
    "preds = np.array(dollar_imp[['Predicted']])\n",
    "acts = np.array(dollar_imp[['Actual']])\n",
    "\n",
    "fraud_impact(amt,preds,acts,falcost=50,show=True,bring=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtr shape:  (28, 170883)\n",
      "ytr shape:  (1, 170883)\n",
      "Xva shape:  (28, 56962)\n",
      "yva shape:  (1, 56962)\n",
      "Xte shape:  (28, 56962)\n",
      "yte shape:  (1, 56962)\n"
     ]
    }
   ],
   "source": [
    "#Now I want to try and improve with basic deep learning.\n",
    "#First drop 'amount' because it causes exploding gradient.\n",
    "nXtr = Xtr.drop(columns=['Amount'])\n",
    "nXva = Xva.drop(columns=['Amount'])\n",
    "nXte = Xte.drop(columns=['Amount'])\n",
    "#Transpose and cast to numpy arrays so the neural net will work.\n",
    "nnXtr = np.array(nXtr).T\n",
    "nnXte = np.array(nXte).T\n",
    "nnytr = np.array(ytr).T\n",
    "nnyte = np.array(yte).T\n",
    "nnXva = np.array(nXva).T\n",
    "nnyva = np.array(yva).T\n",
    "\n",
    "print(\"Xtr shape: \",nnXtr.shape)\n",
    "print(\"ytr shape: \",nnytr.shape)\n",
    "print(\"Xva shape: \",nnXva.shape)\n",
    "print(\"yva shape: \",nnyva.shape)\n",
    "print(\"Xte shape: \",nnXte.shape)\n",
    "print(\"yte shape: \",nnyte.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions for nn.\n",
    "#NN-from-scratch functions;\n",
    "#Helper Functions\n",
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Implements the sigmoid activation in numpy\n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "    Returns:\n",
    "    A -- output of sigmoid(z), same shape as Z\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Implement the RELU function.\n",
    "    Arguments:\n",
    "    Z -- Output of the linear layer, of any shape\n",
    "    Returns:\n",
    "    A -- Post-activation parameter, of the same shape as Z\n",
    "    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    A = np.maximum(0,Z)\n",
    "    assert(A.shape == Z.shape)\n",
    "    cache = Z \n",
    "    return A, cache\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    # When z <= 0, set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "    assert (dZ.shape == Z.shape)\n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    Z = cache\n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    assert (dZ.shape == Z.shape)\n",
    "    return dZ\n",
    "\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    W1 = np.random.randn(n_h, n_x)*0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h)*0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    assert(W1.shape == (n_h, n_x))\n",
    "    assert(b1.shape == (n_h, 1))\n",
    "    assert(W2.shape == (n_y, n_h))\n",
    "    assert(b2.shape == (n_y, 1))\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    return parameters     \n",
    "\n",
    "def initialize_parameters_deep(layers_dims,init):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    initialization = init\n",
    "    parameters = {}\n",
    "    L = len(layers_dims)\n",
    "    \n",
    "    for l in range(1,L):\n",
    "        if initialization=='zeros':\n",
    "            parameters['W' + str(l)] = np.zeros((layers_dims[l],layers_dims[l-1]))\n",
    "            parameters['b' + str(l)] = np.zeros((layers_dims[l],1))\n",
    "        elif initialization=='random':\n",
    "            parameters['W' + str(l)] = np.random.randn(layers_dims[l],layers_dims[l-1])*0.1\n",
    "            parameters['b' + str(l)] = np.zeros((layers_dims[l],1))\n",
    "        elif initialization=='he':\n",
    "            parameters['W' + str(l)] = np.random.randn(layers_dims[l],layers_dims[l-1])*np.sqrt(2./layers_dims[l-1])\n",
    "            parameters['b' + str(l)] = np.zeros((layers_dims[l],1))\n",
    "        elif initialization[0]=='custom':\n",
    "            parameters=initialization[1]\n",
    "        else:\n",
    "            print('init method does not match known methods')\n",
    "            return\n",
    "    return parameters\n",
    "            \n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    Z = W.dot(A) + b\n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    return Z, cache\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    if activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    elif activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    return A, cache\n",
    "\n",
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n",
    "                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n",
    "    \"\"\"\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = \"relu\")\n",
    "        caches.append(cache)\n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation = \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    assert(AL.shape == (1,X.shape[1]))      \n",
    "    return AL, caches\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    # Compute loss from aL and y.\n",
    "    cost = (1./m) * (-np.dot(Y,np.log(AL).T) - np.dot(1-Y, np.log(1-AL).T))\n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    return cost\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    dW = 1./m * np.dot(dZ,A_prev.T)\n",
    "    db = 1./m * np.sum(dZ, axis = 1, keepdims = True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)    \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (there are (L-1) or them, indexes from 0 to L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (there is one, index L-1)\n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    # Initializing the backpropagation\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"AL, Y, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"sigmoid\")\n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 2)], current_cache, activation = \"relu\")\n",
    "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "    return grads\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]  \n",
    "    return parameters\n",
    "\n",
    "def predict(X, y, parameters,threshold=0.5):\n",
    "    \"\"\"\n",
    "    This function is used to predict the results of a  L-layer neural network.\n",
    "    Arguments:\n",
    "    X -- data set of examples you would like to label\n",
    "    parameters -- parameters of the trained model\n",
    "    Returns:\n",
    "    p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2 # number of layers in the neural network\n",
    "    p = np.zeros((1,m))\n",
    "    # Forward propagation\n",
    "    probas, caches = L_model_forward(X, parameters)\n",
    "    # convert probas to 0/1 predictions\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > threshold:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "    #print results\n",
    "    #print (\"predictions: \" + str(p))\n",
    "    #print (\"true labels: \" + str(y))\n",
    "    print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n",
    "    return p\n",
    "    \n",
    "    #Model Function\n",
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False,dynamic_lr=False,init='random'):\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization. (â‰ˆ 1 line of code)\n",
    "    parameters = initialize_parameters_deep(layers_dims,init)\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "        \n",
    "        #Dynamic Learning Rate:\n",
    "        if dynamic_lr ==True:\n",
    "            if float(num_iterations)*0.2==i:\n",
    "                learning_rate=learning_rate*0.7\n",
    "                print(\"LR reduced 1x\")\n",
    "            if float(num_iterations)*0.4==i:\n",
    "                learning_rate=learning_rate*0.8\n",
    "                print(\"LR reduced 2x\")\n",
    "            if float(num_iterations)*0.6==i:\n",
    "                learning_rate=learning_rate*0.9\n",
    "                print(\"LR reduced 3x\")\n",
    "            if float(num_iterations)*0.8==i:\n",
    "                learning_rate=learning_rate*0.95\n",
    "                print(\"LR reduced 4x\")\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        \n",
    "        # Compute cost.\n",
    "        cost = compute_cost(AL, Y)\n",
    "    \n",
    "        # Backward propagation.\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    " \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 170883)\n",
      "(1, 170883)\n"
     ]
    }
   ],
   "source": [
    "print(nnXtr.shape)\n",
    "print(nnytr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.254461\n",
      "Cost after iteration 100: 0.252886\n",
      "Cost after iteration 200: 0.251330\n",
      "Cost after iteration 300: 0.249792\n",
      "Cost after iteration 400: 0.248272\n",
      "Cost after iteration 500: 0.246770\n",
      "Cost after iteration 600: 0.245286\n",
      "Cost after iteration 700: 0.243819\n",
      "Cost after iteration 800: 0.242369\n",
      "Cost after iteration 900: 0.240935\n",
      "Cost after iteration 1000: 0.239518\n",
      "Cost after iteration 1100: 0.238117\n",
      "Cost after iteration 1200: 0.236732\n",
      "Cost after iteration 1300: 0.235362\n",
      "Cost after iteration 1400: 0.234008\n",
      "Cost after iteration 1500: 0.232669\n",
      "Cost after iteration 1600: 0.231344\n",
      "Cost after iteration 1700: 0.230035\n",
      "Cost after iteration 1800: 0.228739\n",
      "Cost after iteration 1900: 0.227458\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#First let's try 1 hidden layer.\n",
    "#The first dimension has to be 28 to match the width of Xtr.\n",
    "layers_dims = [28, 10, 1]\n",
    "param_nn6 = L_layer_model(nnXtr, nnytr, layers_dims, num_iterations = 2000, learning_rate = .0001, print_cost = True, dynamic_lr=False,init=['custom',param_nn5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9977118847398512\n",
      "(170883, 1)\n",
      "77.0\n",
      "314\n"
     ]
    }
   ],
   "source": [
    "#In-sample performance\n",
    "predictions_test = predict(nnXtr, nnytr, param_nn4)\n",
    "ypreds=predictions_test.T\n",
    "print(ypreds.shape)\n",
    "print(np.sum(ypreds))\n",
    "print(np.sum(nnytr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.998174221410765\n",
      "(56962, 1)\n",
      "26.0\n",
      "78\n"
     ]
    }
   ],
   "source": [
    "#out-of-sample performance\n",
    "predictions_test2 = predict(nnXva,nnyva,param_nn4)\n",
    "ypreds=predictions_test2.T\n",
    "print(ypreds.shape)\n",
    "print(np.sum(ypreds))\n",
    "print(np.sum(nnyva))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Model      |      Acc |      AUC |   Precision |   Recall |   F1 Score |\n",
      "|------------+----------+----------+-------------+----------+------------|\n",
      "| 1-Layer FC | 0.998174 | 0.499771 |           0 |        0 |          0 |\n"
     ]
    }
   ],
   "source": [
    "yvanp = np.array(yva)\n",
    "f1 = f1_score(yvanp,ypreds)\n",
    "prec = precision_score(yvanp,ypreds)\n",
    "acc = accuracy_score(yvanp,ypreds)\n",
    "rec = recall_score(yvanp,ypreds)\n",
    "auc = roc_auc_score(yvanp,ypreds)\n",
    "\n",
    "nn_stats = ['1-Layer FC',acc,auc,prec,rec,f1]\n",
    "table_headers = ['Model','Acc','AUC','Precision','Recall','F1 Score']\n",
    "\n",
    "print(tabulate([nn_stats], headers=table_headers, tablefmt='orgtbl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.049330\n",
      "Cost after iteration 100: 0.047488\n",
      "Cost after iteration 200: 0.045810\n",
      "Cost after iteration 300: 0.044277\n",
      "Cost after iteration 400: 0.042873\n",
      "Cost after iteration 500: 0.041585\n",
      "Cost after iteration 600: 0.040399\n",
      "Cost after iteration 700: 0.039304\n",
      "Cost after iteration 800: 0.038292\n",
      "Cost after iteration 900: 0.037355\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Looks like this first model is just not capable of learning what it needs to learn.  Let's try a deeper network - 4 hidden layers.\n",
    "layers_dims = [28,20,15,10,5,1]\n",
    "param_dnn9 = L_layer_model(nnXtr, nnytr, layers_dims, num_iterations = 1000, learning_rate = .0009, print_cost = True, dynamic_lr=False,init=['custom',param_dnn8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9981624854432566\n",
      "in-sample performance:\n",
      "(170883, 1)\n",
      "0.0\n",
      "314\n",
      "| Model   |      Acc |   AUC |   Precision |   Recall |   F1 Score |\n",
      "|---------+----------+-------+-------------+----------+------------|\n",
      "| FC-4 IS | 0.998162 |   0.5 |           0 |        0 |          0 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexm\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\alexm\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9986306660580737\n",
      "OOS performance\n",
      "(56962, 1)\n",
      "0.0\n",
      "78\n",
      "| Model    |      Acc |   AUC |   Precision |   Recall |   F1 Score |\n",
      "|----------+----------+-------+-------------+----------+------------|\n",
      "| FC-4 OOS | 0.998631 |   0.5 |           0 |        0 |          0 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexm\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\alexm\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#In-sample performance\n",
    "predictions_test = predict(nnXtr, nnytr, param_dnn9)\n",
    "ypreds=predictions_test.T\n",
    "print(\"in-sample performance:\")\n",
    "print(ypreds.shape)\n",
    "print(np.sum(ypreds))\n",
    "print(np.sum(nnytr))\n",
    "yvanp = np.array(ytr)\n",
    "f1 = f1_score(yvanp,ypreds)\n",
    "prec = precision_score(yvanp,ypreds)\n",
    "acc = accuracy_score(yvanp,ypreds)\n",
    "rec = recall_score(yvanp,ypreds)\n",
    "auc = roc_auc_score(yvanp,ypreds)\n",
    "\n",
    "nn_stats = ['FC-4 IS',acc,auc,prec,rec,f1]\n",
    "table_headers = ['Model','Acc','AUC','Precision','Recall','F1 Score']\n",
    "\n",
    "print(tabulate([nn_stats], headers=table_headers, tablefmt='orgtbl'))\n",
    "#out-of-sample performance\n",
    "predictions_test2 = predict(nnXva,nnyva,param_dnn9)\n",
    "ypreds=predictions_test2.T\n",
    "print(\"OOS performance\")\n",
    "print(ypreds.shape)\n",
    "print(np.sum(ypreds))\n",
    "print(np.sum(nnyva))\n",
    "yvanp = np.array(yva)\n",
    "f1 = f1_score(yvanp,ypreds)\n",
    "prec = precision_score(yvanp,ypreds)\n",
    "acc = accuracy_score(yvanp,ypreds)\n",
    "rec = recall_score(yvanp,ypreds)\n",
    "auc = roc_auc_score(yvanp,ypreds)\n",
    "\n",
    "nn_stats = ['FC-4 OOS',acc,auc,prec,rec,f1]\n",
    "table_headers = ['Model','Acc','AUC','Precision','Recall','F1 Score']\n",
    "\n",
    "print(tabulate([nn_stats], headers=table_headers, tablefmt='orgtbl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The data is just too imbalanced...\n",
    "#Let's try to undersample with \"Near Miss 1\".\n",
    "from collections import Counter\n",
    "from imblearn.under_sampling import NearMiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nm1 = NearMiss(version=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr_resamp, ytr_resamp = nm1.fit_resample(Xtr, ytr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtr: (170883, 29)\n",
      "ytr: (170883, 1)\n",
      "Xtr_resamp: (628, 29)\n",
      "ytr_resamp: (628, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Xtr:\",Xtr.shape)\n",
    "print(\"ytr:\",ytr.shape)\n",
    "print(\"Xtr_resamp:\",Xtr_resamp.shape)\n",
    "print(\"ytr_resamp:\",ytr_resamp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>628.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.500399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Class\n",
       "count  628.000000\n",
       "mean     0.500000\n",
       "std      0.500399\n",
       "min      0.000000\n",
       "25%      0.000000\n",
       "50%      0.500000\n",
       "75%      1.000000\n",
       "max      1.000000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now we have 628 examples with perfect balance;\n",
    "ytr_resamp.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtr shape:  (28, 628)\n",
      "ytr shape:  (1, 628)\n",
      "Xte shape:  (28, 56962)\n",
      "yte shape:  (1, 56962)\n"
     ]
    }
   ],
   "source": [
    "#I'm going to train it on 628 examples and test it on 56,962 examples.\n",
    "nXtr = Xtr_resamp.drop(columns=['Amount'])\n",
    "#Transpose and cast to numpy arrays so the neural net will work.\n",
    "nnXtr = np.array(nXtr).T\n",
    "nnytr = np.array(ytr_resamp).T\n",
    "\n",
    "print(\"Xtr shape: \",nnXtr.shape)\n",
    "print(\"ytr shape: \",nnytr.shape)\n",
    "print(\"Xte shape: \",nnXte.shape)\n",
    "print(\"yte shape: \",nnyte.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we can go much deeper since the data is so small. 7 hidden layers.\n",
    "layers_dims = [28,20,15,12,10,8,6,4,1]\n",
    "param_unn = L_layer_model(nnXtr, nnytr, layers_dims, num_iterations = 15000, learning_rate = .0001, print_cost = True, dynamic_lr=True,init='he')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_param = param_unn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_unn2 = L_layer_model(nnXtr, nnytr, layers_dims, num_iterations = 5000, learning_rate = .00005, print_cost = True, dynamic_lr=True,init=['custom',param_unn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_param = param_unn2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_unn3 = L_layer_model(nnXtr, nnytr, layers_dims, num_iterations = 5000, learning_rate = .00005, print_cost = True, dynamic_lr=True,init=['custom',param_unn2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9394904458598726\n",
      "in-sample performance:\n",
      "(628, 1)\n",
      "(628, 1)\n",
      "322.0\n",
      "314\n",
      "| Model    |     Acc |     AUC |   Precision |   Recall |   F1 Score |\n",
      "|----------+---------+---------+-------------+----------+------------|\n",
      "| FC-US IS | 0.93949 | 0.93949 |    0.928571 | 0.952229 |   0.940252 |\n"
     ]
    }
   ],
   "source": [
    "predictions_test = predict(nnXtr,nnytr,param_unn3)\n",
    "ypreds=predictions_test.T\n",
    "yacts = nnytr.T\n",
    "print(\"in-sample performance:\")\n",
    "print(ypreds.shape)\n",
    "print(yacts.shape)\n",
    "print(np.sum(ypreds))\n",
    "print(np.sum(yacts))\n",
    "\n",
    "#In-sample performance\n",
    "f1 = f1_score(yacts,ypreds)\n",
    "prec = precision_score(yacts,ypreds)\n",
    "acc = accuracy_score(yacts,ypreds)\n",
    "rec = recall_score(yacts,ypreds)\n",
    "auc = roc_auc_score(yacts,ypreds)\n",
    "\n",
    "nn_stats = ['FC-US IS',acc,auc,prec,rec,f1]\n",
    "table_headers = ['Model','Acc','AUC','Precision','Recall','F1 Score']\n",
    "\n",
    "print(tabulate([nn_stats], headers=table_headers, tablefmt='orgtbl'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3739861662160739\n",
      "OOS performance\n",
      "(56962, 1)\n",
      "(56962, 1)\n",
      "35753.0\n",
      "100\n",
      "| Model     |      Acc |      AUC |   Precision |   Recall |   F1 Score |\n",
      "|-----------+----------+----------+-------------+----------+------------|\n",
      "| FC-US OOS | 0.373986 | 0.671469 |  0.00271306 |     0.97 | 0.00541098 |\n"
     ]
    }
   ],
   "source": [
    "#out-of-sample performance\n",
    "predictions_test2 = predict(nnXte,nnyte,param_unn3)\n",
    "ypreds=predictions_test2.T\n",
    "yacts = nnyte.T\n",
    "print(\"OOS performance\")\n",
    "print(ypreds.shape)\n",
    "print(yacts.shape)\n",
    "print(np.sum(ypreds))\n",
    "print(np.sum(yacts))\n",
    "f1 = f1_score(yacts,ypreds)\n",
    "prec = precision_score(yacts,ypreds)\n",
    "acc = accuracy_score(yacts,ypreds)\n",
    "rec = recall_score(yacts,ypreds)\n",
    "auc = roc_auc_score(yacts,ypreds)\n",
    "\n",
    "nn_stats = ['FC-US OOS',acc,auc,prec,rec,f1]\n",
    "table_headers = ['Model','Acc','AUC','Precision','Recall','F1 Score']\n",
    "\n",
    "print(tabulate([nn_stats], headers=table_headers, tablefmt='orgtbl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtr shape:  (28, 170883)\n",
      "ytr shape:  (1, 170883)\n",
      "Xva shape:  (28, 56962)\n",
      "yva shape:  (1, 56962)\n",
      "Xte shape:  (28, 56962)\n",
      "yte shape:  (1, 56962)\n"
     ]
    }
   ],
   "source": [
    "#Not a good result.  \n",
    "#The model needs more negatives to learn from.\n",
    "#Let's try a combination of oversampling and undersampling.\n",
    "nXtr = Xtr.drop(columns=['Amount'])\n",
    "nXva = Xva.drop(columns=['Amount'])\n",
    "nXte = Xte.drop(columns=['Amount'])\n",
    "#Transpose and cast to numpy arrays so the neural net will work.\n",
    "nnXtr = np.array(nXtr).T\n",
    "nnXte = np.array(nXte).T\n",
    "nnytr = np.array(ytr).T\n",
    "nnyte = np.array(yte).T\n",
    "nnXva = np.array(nXva).T\n",
    "nnyva = np.array(yva).T\n",
    "\n",
    "print(\"Xtr shape: \",nnXtr.shape)\n",
    "print(\"ytr shape: \",nnytr.shape)\n",
    "print(\"Xva shape: \",nnXva.shape)\n",
    "print(\"yva shape: \",nnyva.shape)\n",
    "print(\"Xte shape: \",nnXte.shape)\n",
    "print(\"yte shape: \",nnyte.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, undersampling Xtr is easy with pandas.\n",
    "comb = Xtr\n",
    "comb[['Class']]=ytr[['Class']]\n",
    "comb_smaller = comb.sample(n=80000,random_state=35)\n",
    "#Xtr_smaller = Xtr.sample(n=80000)\n",
    "#Xtr_smaller.drop(columns=['Amount'],inplace=True)\n",
    "#nnXtr_smaller=np.array(Xtr_smaller).T\n",
    "#nnXtr_smaller.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80000, 30)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comb_smaller.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split back out into X and y.\n",
    "comby = comb_smaller[['Class']]\n",
    "combX = comb_smaller.drop(columns=['Class','Amount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80000, 28)\n",
      "(80000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(combX.shape)\n",
    "print(comby.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now oversample the frauds using SMOTE interpolation.\n",
    "from imblearn.over_sampling import SMOTE\n",
    "Xtr_resamp, ytr_resamp = SMOTE().fit_resample(combX, comby)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159662, 28)\n",
      "(159662, 1)\n"
     ]
    }
   ],
   "source": [
    "print(Xtr_resamp.shape)\n",
    "print(ytr_resamp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>159662.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.500002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Class\n",
       "count  159662.000000\n",
       "mean        0.500000\n",
       "std         0.500002\n",
       "min         0.000000\n",
       "25%         0.000000\n",
       "50%         0.500000\n",
       "75%         1.000000\n",
       "max         1.000000"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's try a shallow nn to see if we have a good jumpoff point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtr shape:  (28, 159662)\n",
      "ytr shape:  (1, 159662)\n",
      "Xte shape:  (28, 56962)\n",
      "yte shape:  (1, 56962)\n"
     ]
    }
   ],
   "source": [
    "nnXtr = np.array(Xtr_resamp).T\n",
    "nnytr = np.array(ytr_resamp).T\n",
    "\n",
    "print(\"Xtr shape: \",nnXtr.shape)\n",
    "print(\"ytr shape: \",nnytr.shape)\n",
    "print(\"Xte shape: \",nnXte.shape)\n",
    "print(\"yte shape: \",nnyte.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.412427\n",
      "Cost after iteration 100: 0.409898\n",
      "Cost after iteration 200: 0.407425\n",
      "Cost after iteration 300: 0.405003\n",
      "Cost after iteration 400: 0.402632\n",
      "Cost after iteration 500: 0.400308\n",
      "Cost after iteration 600: 0.398029\n",
      "Cost after iteration 700: 0.395794\n",
      "Cost after iteration 800: 0.393601\n",
      "Cost after iteration 900: 0.391448\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#2 hidden layers this time;\n",
    "layers_dims = [28,15,10,1]\n",
    "param_onn4 = L_layer_model(nnXtr, nnytr, layers_dims, num_iterations = 1000, learning_rate = .0005, print_cost = True, dynamic_lr=False,init=['custom',param_onn3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8560271072640954\n",
      "in-sample performance:\n",
      "(159662, 1)\n",
      "(159662, 1)\n",
      "56910.0\n",
      "79831\n",
      "| Model    |      Acc |      AUC |   Precision |   Recall |   F1 Score |\n",
      "|----------+----------+----------+-------------+----------+------------|\n",
      "| FC-OS IS | 0.856027 | 0.856027 |     0.99942 | 0.712468 |   0.831894 |\n"
     ]
    }
   ],
   "source": [
    "predictions_test = predict(nnXtr,nnytr,param_onn4,threshold=0.88)\n",
    "ypreds=predictions_test.T\n",
    "yacts = nnytr.T\n",
    "print(\"in-sample performance:\")\n",
    "print(ypreds.shape)\n",
    "print(yacts.shape)\n",
    "print(np.sum(ypreds))\n",
    "print(np.sum(yacts))\n",
    "\n",
    "#In-sample performance\n",
    "f1 = f1_score(yacts,ypreds)\n",
    "prec = precision_score(yacts,ypreds)\n",
    "acc = accuracy_score(yacts,ypreds)\n",
    "rec = recall_score(yacts,ypreds)\n",
    "auc = roc_auc_score(yacts,ypreds)\n",
    "\n",
    "nn_stats = ['FC-OS IS',acc,auc,prec,rec,f1]\n",
    "table_headers = ['Model','Acc','AUC','Precision','Recall','F1 Score']\n",
    "\n",
    "print(tabulate([nn_stats], headers=table_headers, tablefmt='orgtbl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.999403110845827\n",
      "OOS performance\n",
      "(56962, 1)\n",
      "(56962, 1)\n",
      "84.0\n",
      "100\n",
      "| Model     |      Acc |      AUC |   Precision |   Recall |   F1 Score |\n",
      "|-----------+----------+----------+-------------+----------+------------|\n",
      "| FC-OS OOS | 0.999403 | 0.874921 |    0.892857 |     0.75 |   0.815217 |\n"
     ]
    }
   ],
   "source": [
    "#out-of-sample performance\n",
    "predictions_test2 = predict(nnXte,nnyte,param_onn4,threshold=0.88)\n",
    "ypreds=predictions_test2.T\n",
    "yacts = nnyte.T\n",
    "print(\"OOS performance\")\n",
    "print(ypreds.shape)\n",
    "print(yacts.shape)\n",
    "print(np.sum(ypreds))\n",
    "print(np.sum(yacts))\n",
    "f1 = f1_score(yacts,ypreds)\n",
    "prec = precision_score(yacts,ypreds)\n",
    "acc = accuracy_score(yacts,ypreds)\n",
    "rec = recall_score(yacts,ypreds)\n",
    "auc = roc_auc_score(yacts,ypreds)\n",
    "\n",
    "nn_stats = ['FC-OS OOS',acc,auc,prec,rec,f1]\n",
    "table_headers = ['Model','Acc','AUC','Precision','Recall','F1 Score']\n",
    "\n",
    "print(tabulate([nn_stats], headers=table_headers, tablefmt='orgtbl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall I learned a lot about dealing with imbalanced data.\n"
     ]
    }
   ],
   "source": [
    "#OK, I kinda cheated by messing with the prediction threshold.  BUT we can see that the last oversampling method worked pretty well.\n",
    "#the model would continue to improve if I trained it for a long time.\n",
    "\n",
    "print(\"Overall I learned a lot about dealing with imbalanced data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
